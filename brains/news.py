"""
–ú–æ–¥—É–ª—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω–æ–≥–æ
- –¢–µ–ª–µ–º–∞—Ç–∏–∫–∞ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞
- –¢–∞—Ö–æ–≥—Ä–∞—Ñ–∏—è
- –í–µ–±–∏–Ω–∞—Ä—ã –∏ –æ–Ω–ª–∞–π–Ω-–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è
- –ò–Ω–Ω–æ–≤–∞—Ü–∏–∏ –æ—Ç—Ä–∞—Å–ª–∏

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–∫–∞–∑–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –ë–î
- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–µ–π –ø–æ URL
- –ü–æ–∫–∞–∑ —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ 1 —á–∞—Å
"""
import httpx
import logging
import xml.etree.ElementTree as ET
import asyncio
import hashlib
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Tuple
from brains.clients import supabase_client
from brains.config import MISTRAL_API_KEY

logger = logging.getLogger(__name__)

# ============================================================================
# –ò–°–¢–û–ß–ù–ò–ö–ò –ù–û–í–û–°–¢–ï–ô
# ============================================================================
# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ RSS-—Ñ–∏–¥—ã —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º–∏ URL
NEWS_SOURCES = [
    {
        "name": "Habr Transport",
        "url": "https://habr.com/ru/rss/hubs/transport/articles/all/?fl=ru",
        "category": "innovations",
        "enabled": True
    },
    {
        "name": "–í–µ—Å—Ç–Ω–∏–∫ –ì–õ–û–ù–ê–°–°",
        "url": "http://vestnik-glonass.ru/rss/",
        "category": "telematics",
        "enabled": True
    },
    {
        "name": "–†–æ—Å–∞–≤—Ç–æ—Ç—Ä–∞–Ω—Å",
        "url": "http://rosavtotrans.ru/ru/news/rss/",
        "category": "tachography",
        "enabled": True
    },
    {
        "name": "TransportRussia",
        "url": "https://www.transrussia.org/rss/news",
        "category": "logistics",
        "enabled": True
    }
]

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
KEYWORDS = [
    "—Ç–µ–ª–µ–º–∞—Ç–∏–∫–∞", "—Ç–∞—Ö–æ–≥—Ä–∞—Ñ", "–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞", "–≥–ª–æ–Ω–∞—Å—Å",
    "gps", "–ª–æ–≥–∏—Å—Ç–∏–∫–∞", "—Ü–∏—Ñ—Ä–æ–≤–∏–∑–∞—Ü–∏—è", "–≤–µ–±–∏–Ω–∞—Ä", "–æ–Ω–ª–∞–π–Ω-–≤—Å—Ç—Ä–µ—á–∞",
    "–º–∏–Ω—Ç—Ä–∞–Ω—Å", "–∞–≤—Ç–æ–ø–∞—Ä–∫", "—Å–∫—É–¥", "—É—á–µ—Ç —Ç–æ–ø–ª–∏–≤–∞", "—ç—Ä–∞-–≥–ª–æ–Ω–∞—Å—Å"
]

# –ö–∞–ª–µ–Ω–¥–∞—Ä—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π 2026
INDUSTRY_EVENTS_2026 = [
    {"date": "24-25.03.2026", "title": "–ò–¢–° —Ä–µ–≥–∏–æ–Ω–∞–º 2026 (–¢—É–ª–∞)", "desc": "–¢–µ–ª–µ–º–∞—Ç–∏–∫–∞, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, –ò–¢–°"},
    {"date": "01-03.04.2026", "title": "–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ-–ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ñ–æ—Ä—É–º (–°–ü–±)", "desc": "B2B –ª–æ–≥–∏—Å—Ç–∏–∫–∞ –∏ IT"},
    {"date": "02-03.04.2026", "title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–≤—Ç–æ–ø–∞—Ä–∫–æ–º 2026 (–ê—Å—Ç–∞–Ω–∞)", "desc": "–ò–ò –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —Ñ–ª–æ—Ç–∞"},
    {"date": "09-11.06.2026", "title": "–≠–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞-–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç 2026 (–ú–æ—Å–∫–≤–∞)", "desc": "–ù–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã –Ω–∞ –í–î–ù–•"},
    {"date": "–ê–ø—Ä–µ–ª—å 2026", "title": "–ù–ê–í–ò–¢–ï–•-2026", "desc": "–ì–ª–∞–≤–Ω–∞—è –≤—ã—Å—Ç–∞–≤–∫–∞ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –≤ –†–§"}
]

# –ö—ç—à –Ω–æ–≤–æ—Å—Ç–µ–π
NEWS_CACHE = {
    "news": None,
    "expire_at": None
}


# ============================================================================
# –†–ê–ë–û–¢–ê –° –ë–î
# ============================================================================

async def get_shown_news_links(limit: int = 100) -> set:
    """–ü–æ–ª—É—á–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ URL —É–∂–µ –ø–æ–∫–∞–∑–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
    try:
        response = supabase_client.table("news_history")\
            .select("link")\
            .order("shown_at", desc=True)\
            .limit(limit)\
            .execute()
        
        if response.data:
            return {item["link"] for item in response.data}
        return set()
    except Exception as e:
        logger.error(f"Error getting shown news: {e}")
        return set()


async def get_news_history_count(days: int = 7) -> int:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–µ—Ä–∏–æ–¥"""
    try:
        from datetime import timedelta
        cutoff = datetime.now(timezone.utc) - timedelta(days=days)
        
        response = supabase_client.table("news_history")\
            .select("id", count="exact")\
            .gte("shown_at", cutoff.isoformat())\
            .execute()
        
        return response.count if hasattr(response, 'count') else 0
    except Exception as e:
        logger.error(f"Error getting news count: {e}")
        return 0


async def save_news_to_history(news_items: List[Dict], user_id: int = 0):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–∫–∞–∑–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ –∏—Å—Ç–æ—Ä–∏—é"""
    if not news_items:
        return
    
    try:
        records = []
        for news in news_items:
            records.append({
                "title": news["title"],
                "link": news["link"],
                "source": news["source"],
                "category": news.get("category", "general"),
                "published_at": news.get("published_at"),
                "user_id": user_id
            })
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º upsert —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–µ–π
        for record in records:
            try:
                supabase_client.table("news_history")\
                    .upsert(record, on_conflict="link")\
                    .execute()
            except Exception as e:
                logger.debug(f"News already exists: {record['link']}")
        
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(records)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –∏—Å—Ç–æ—Ä–∏—é")
    except Exception as e:
        logger.error(f"Error saving news history: {e}")


async def clear_old_news_history(days: int = 30):
    """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—É—é –∏—Å—Ç–æ—Ä–∏—é –Ω–æ–≤–æ—Å—Ç–µ–π"""
    try:
        from datetime import timedelta
        cutoff = datetime.now(timezone.utc) - timedelta(days=days)
        
        response = supabase_client.table("news_history")\
            .delete()\
            .lt("shown_at", cutoff.isoformat())\
            .execute()
        
        logger.info(f"üßπ –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(response.data) if response.data else 0}")
        return len(response.data) if response.data else 0
    except Exception as e:
        logger.error(f"Error clearing old news: {e}")
        return 0


# ============================================================================
# –ü–ê–†–°–ò–ù–ì RSS
# ============================================================================

def extract_publication_date(item) -> Optional[datetime]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏–∑ RSS —ç–ª–µ–º–µ–Ω—Ç–∞"""
    date_fields = ['pubDate', 'published', 'updated', 'created']
    
    for field in date_fields:
        date_elem = item.find(field)
        if date_elem is not None and date_elem.text:
            try:
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
                date_str = date_elem.text
                # RFC 822 —Ñ–æ—Ä–º–∞—Ç
                if 'GMT' in date_str or '+' in date_str:
                    from email.utils import parsedate_to_datetime
                    return parsedate_to_datetime(date_str)
                # ISO —Ñ–æ—Ä–º–∞—Ç
                return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
            except Exception:
                continue
    return datetime.now(timezone.utc)


async def fetch_rss(client: httpx.AsyncClient, source: Dict) -> List[Dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω RSS –∏—Å—Ç–æ—á–Ω–∏–∫"""
    try:
        response = await client.get(source["url"], timeout=15.0, follow_redirects=True)
        
        if response.status_code != 200:
            logger.warning(f"‚ùå –ò—Å—Ç–æ—á–Ω–∏–∫ {source['name']} –≤–µ—Ä–Ω—É–ª {response.status_code}")
            return []
        
        root = ET.fromstring(response.text)
        items = []
        
        # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã item –∏–ª–∏ entry (–¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ RSS)
        rss_items = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry')
        
        for item in rss_items[:15]:  # –ë–µ—Ä—ë–º –º–∞–∫—Å–∏–º—É–º 15 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö
            title_elem = item.find('title')
            link_elem = item.find('link')
            
            # –î–ª—è Atom —Ñ–æ—Ä–º–∞—Ç–∞ link —ç—Ç–æ —ç–ª–µ–º–µ–Ω—Ç —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º href
            if link_elem is not None and link_elem.text is None:
                link_elem = item.find('{http://www.w3.org/2005/Atom}link')
                if link_elem is not None and 'href' in link_elem.attrib:
                    link = link_elem.attrib['href']
                else:
                    continue
            else:
                link = link_elem.text if link_elem is not None else None
            
            if title_elem is None or not title_elem.text or not link:
                continue
            
            title = title_elem.text.strip()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            match_score = sum(1 for kw in KEYWORDS if kw.lower() in title.lower())
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            pub_date = extract_publication_date(item)
            
            items.append({
                "title": title,
                "link": link,
                "source": source["name"],
                "category": source["category"],
                "score": match_score,
                "published_at": pub_date.isoformat() if pub_date else None
            })
        
        return items
        
    except httpx.TimeoutException:
        logger.warning(f"‚åõÔ∏è –¢–∞–π–º–∞—É—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source['name']}")
        return []
    except Exception as e:
        logger.warning(f"‚ùå –û—à–∏–±–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source['name']}: {e}")
        return []


# ============================================================================
# –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê
# ============================================================================

async def get_latest_news(limit: int = 5, force_refresh: bool = False, user_id: int = 0) -> str:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —É–∂–µ –ø–æ–∫–∞–∑–∞–Ω–Ω—ã–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç
    –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
    
    Args:
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π
        force_refresh: –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—ç—à
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
    """
    moscow_tz = timezone(timedelta(hours=3))
    now = datetime.now(moscow_tz)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ (–µ—Å–ª–∏ –Ω–µ force_refresh)
    if not force_refresh and NEWS_CACHE["news"] and NEWS_CACHE["expire_at"]:
        if now < NEWS_CACHE["expire_at"]:
            logger.debug("üì∞ –ù–æ–≤–æ—Å—Ç–∏: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à")
            return NEWS_CACHE["news"]
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–∂–µ –ø–æ–∫–∞–∑–∞–Ω–Ω—ã—Ö URL
    shown_links = await get_shown_news_links(limit=200)
    logger.info(f"üì∞ –ù–∞–π–¥–µ–Ω–æ {len(shown_links)} —É–∂–µ –ø–æ–∫–∞–∑–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
    
    # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    all_news = []
    enabled_sources = [s for s in NEWS_SOURCES if s.get("enabled", True)]
    
    async with httpx.AsyncClient(follow_redirects=True) as client:
        tasks = [fetch_rss(client, source) for source in enabled_sources]
        results = await asyncio.gather(*tasks)
        
        for res in results:
            all_news.extend(res)
    
    logger.info(f"üì∞ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(all_news)}")
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –¥—É–±–ª–∏ –∏ —É–∂–µ –ø–æ–∫–∞–∑–∞–Ω–Ω—ã–µ
    unique_news = []
    seen_links = set()
    
    for news in all_news:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ URL —É–∂–µ –µ—Å—Ç—å –≤ —Ç–µ–∫—É—â–µ–π –≤—ã–±–æ—Ä–∫–µ
        if news["link"] in seen_links:
            continue
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —É–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–ª–∏
        if news["link"] in shown_links:
            continue
        
        seen_links.add(news["link"])
        unique_news.append(news)
    
    logger.info(f"üì∞ –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(unique_news)}")
    
    # –ï—Å–ª–∏ –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç
    if not unique_news:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ –ø–æ–∫–∞–∑—ã–≤–∞–ª–∏ –Ω–æ–≤–æ—Å—Ç–∏
        count = await get_news_history_count(days=1)
        if count > 0:
            message = "üì∞ –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è –Ω–µ—Ç. –í—Å—ë –≤–∞–∂–Ω–æ–µ —è —É–∂–µ —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–ª–∞! üòä"
        else:
            message = "üì∞ –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ–∫–∞ –Ω–µ—Ç. –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –∏–ª–∏ –≤ –º–∏—Ä–µ —Ç–µ–ª–µ–º–∞—Ç–∏–∫–∏ —Ç–∏—Ö–æ... ‚òï"
        
        NEWS_CACHE["news"] = message
        NEWS_CACHE["expire_at"] = now + timedelta(minutes=30)
        return message
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –¥–∞—Ç–µ
    unique_news.sort(key=lambda x: (x["score"], x.get("published_at", "")), reverse=True)
    
    # –ë–µ—Ä—ë–º —Ç–æ–ø
    top_news = unique_news[:limit]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
    await save_news_to_history(top_news, user_id)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç
    report = await format_news_report(top_news)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –Ω–∞ 1 —á–∞—Å
    NEWS_CACHE["news"] = report
    NEWS_CACHE["expire_at"] = now + timedelta(hours=1)
    
    return report


async def format_news_report(news_list: List[Dict]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –≤ –∫—Ä–∞—Å–∏–≤—ã–π –æ—Ç—á—ë—Ç –¥–ª—è Telegram"""
    if not news_list:
        return "–ù–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç."
    
    report = ["üóû **–°–≤–µ–∂–µ–µ –≤ —Ç–µ–ª–µ–º–∞—Ç–∏–∫–µ –∏ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–µ:**\n"]
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    categories_order = ["telematics", "tachography", "innovations", "logistics"]
    category_names = {
        "telematics": "üõ∞ –¢–µ–ª–µ–º–∞—Ç–∏–∫–∞ –∏ –ì–õ–û–ù–ê–°–°",
        "tachography": "üìä –¢–∞—Ö–æ–≥—Ä–∞—Ñ–∏—è –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å",
        "innovations": "üí° –ò–Ω–Ω–æ–≤–∞—Ü–∏–∏",
        "logistics": "üöö –õ–æ–≥–∏—Å—Ç–∏–∫–∞"
    }
    
    for category in categories_order:
        cat_news = [n for n in news_list if n.get("category") == category][:2]
        
        if cat_news:
            header = category_names.get(category, "üì∞ –ù–æ–≤–æ—Å—Ç–∏")
            report.append(f"*{header}*")
            
            for n in cat_news:
                # –°–æ–∫—Ä–∞—â–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                title = n["title"]
                if len(title) > 100:
                    title = title[:97] + "..."
                
                report.append(f"üîπ {title}")
                report.append(f"üîó {n['link']}")
            
            report.append("")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è
    today = datetime.now()
    upcoming_events = []
    
    for ev in INDUSTRY_EVENTS_2026:
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ
        upcoming_events.append(ev)
    
    if upcoming_events:
        report.append("üìÖ **–ë–ª–∏–∂–∞–π—à–∏–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è 2026:**")
        for ev in upcoming_events[:3]:
            report.append(f"üìç {ev['date']} ‚Äî {ev['title']}")
        report.append("")
    
    report.append("_–•–æ—á–µ—à—å —É–∑–Ω–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ? –ü—Ä–æ—Å—Ç–æ —Å–ø—Ä–æ—Å–∏!_")
    
    return "\n".join(report)


# ============================================================================
# –£–ü–†–ê–í–õ–ï–ù–ò–ï –ò–°–¢–û–ß–ù–ò–ö–ê–ú–ò
# ============================================================================

async def get_news_sources() -> List[Dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    return NEWS_SOURCES.copy()


async def enable_source(source_name: str) -> bool:
    """–í–∫–ª—é—á–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫"""
    for source in NEWS_SOURCES:
        if source["name"].lower() == source_name.lower():
            source["enabled"] = True
            logger.info(f"‚úÖ –ò—Å—Ç–æ—á–Ω–∏–∫ '{source_name}' –≤–∫–ª—é—á–µ–Ω")
            return True
    return False


async def disable_source(source_name: str) -> bool:
    """–û—Ç–∫–ª—é—á–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫"""
    for source in NEWS_SOURCES:
        if source["name"].lower() == source_name.lower():
            source["enabled"] = False
            logger.info(f"‚è∏Ô∏è –ò—Å—Ç–æ—á–Ω–∏–∫ '{source_name}' –æ—Ç–∫–ª—é—á–µ–Ω")
            return True
    return False


async def add_custom_source(name: str, url: str, category: str) -> bool:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫"""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç URL
    for source in NEWS_SOURCES:
        if source["url"] == url:
            return False
    
    NEWS_SOURCES.append({
        "name": name,
        "url": url,
        "category": category,
        "enabled": True
    })
    
    logger.info(f"üì∞ –î–æ–±–∞–≤–ª–µ–Ω –∏—Å—Ç–æ—á–Ω–∏–∫: {name}")
    return True


# ============================================================================
# –û–ß–ò–°–¢–ö–ê –ö–≠–®–ê
# ============================================================================

def clear_news_cache():
    """–û—á–∏—â–∞–µ—Ç –∫—ç—à –Ω–æ–≤–æ—Å—Ç–µ–π"""
    NEWS_CACHE["news"] = None
    NEWS_CACHE["expire_at"] = None
    logger.info("üßπ –ö—ç—à –Ω–æ–≤–æ—Å—Ç–µ–π –æ—á–∏—â–µ–Ω")
